{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "992c99fb"
      },
      "source": [
        "# download libraries\n"
      ],
      "id": "992c99fb"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXJEL1iOp8o2",
        "outputId": "bbc126b3-9f17-4ee2-a432-ceeddf1c6f5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rtdl\n",
            "  Downloading rtdl-0.0.13-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.18 in /usr/local/lib/python3.7/dist-packages (from rtdl) (1.21.6)\n",
            "Requirement already satisfied: torch<2,>=1.7 in /usr/local/lib/python3.7/dist-packages (from rtdl) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.7->rtdl) (4.1.1)\n",
            "Installing collected packages: rtdl\n",
            "Successfully installed rtdl-0.0.13\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting libzero==0.0.4\n",
            "  Downloading libzero-0.0.4-py3-none-any.whl (26 kB)\n",
            "Collecting pynvml<9,>=8.0\n",
            "  Downloading pynvml-8.0.4-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: tqdm<5,>=4.0 in /usr/local/lib/python3.7/dist-packages (from libzero==0.0.4) (4.64.0)\n",
            "Requirement already satisfied: numpy<2,>=1.17 in /usr/local/lib/python3.7/dist-packages (from libzero==0.0.4) (1.21.6)\n",
            "Requirement already satisfied: torch<2,>=1.6 in /usr/local/lib/python3.7/dist-packages (from libzero==0.0.4) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.6->libzero==0.0.4) (4.1.1)\n",
            "Installing collected packages: pynvml, libzero\n",
            "Successfully installed libzero-0.0.4 pynvml-8.0.4\n"
          ]
        }
      ],
      "source": [
        "# Requirements:\n",
        "!pip install rtdl\n",
        "!pip install libzero==0.0.4 \n"
      ],
      "id": "HXJEL1iOp8o2"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUCS9tuC_csH",
        "outputId": "eaecf02a-af76-46b1-8e35-b0835aee2570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openml\n",
            "  Downloading openml-0.12.2.tar.gz (119 kB)\n",
            "\u001b[K     |████████████████████████████████| 119 kB 17.1 MB/s \n",
            "\u001b[?25hCollecting liac-arff>=2.4.0\n",
            "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from openml) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from openml) (1.0.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from openml) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from openml) (1.3.5)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from openml) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from openml) (1.21.6)\n",
            "Collecting minio\n",
            "  Downloading minio-7.1.11-py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from openml) (6.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->openml) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->openml) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->openml) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->openml) (3.1.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from minio->openml) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from minio->openml) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->openml) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->openml) (2.10)\n",
            "Building wheels for collected packages: openml, liac-arff\n",
            "  Building wheel for openml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openml: filename=openml-0.12.2-py3-none-any.whl size=137326 sha256=a1eb2ead786bc32634726a10d68761a7e1a55714ec260d27eb601762432e9adb\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/20/88/cf4ac86aa18e2cd647ed16ebe274a5dacee9d0075fa02af250\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11732 sha256=cfe2e3e178eead452c149537ffc8f8a8c53efb7c00e8aaa69eb126fe888dd428\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/0f/15/332ca86cbebf25ddf98518caaf887945fbe1712b97a0f2493b\n",
            "Successfully built openml liac-arff\n",
            "Installing collected packages: xmltodict, minio, liac-arff, openml\n",
            "Successfully installed liac-arff-2.5.0 minio-7.1.11 openml-0.12.2 xmltodict-0.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install openml"
      ],
      "id": "FUCS9tuC_csH"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGlipvj4HUua",
        "outputId": "54f3fa8c-9188-48b3-dc50-2c289e73ee7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.1-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 31.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 66.3 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 64.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 71.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 77.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 72.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 51.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 72.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=f125d489e5d44c4746c5d9be0bedcf8332ea731542abe1034b9c004669de0a8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n"
      ],
      "id": "nGlipvj4HUua"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGuoK2CCdiH8",
        "outputId": "792e10b6-46c2-4f6e-f472-44fa38ff3c18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rtdl in /usr/local/lib/python3.7/dist-packages (0.0.13)\n",
            "Requirement already satisfied: torch<2,>=1.7 in /usr/local/lib/python3.7/dist-packages (from rtdl) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy<2,>=1.18 in /usr/local/lib/python3.7/dist-packages (from rtdl) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.7->rtdl) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: libzero==0.0.4 in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied: pynvml<9,>=8.0 in /usr/local/lib/python3.7/dist-packages (from libzero==0.0.4) (8.0.4)\n",
            "Requirement already satisfied: tqdm<5,>=4.0 in /usr/local/lib/python3.7/dist-packages (from libzero==0.0.4) (4.64.0)\n",
            "Requirement already satisfied: torch<2,>=1.6 in /usr/local/lib/python3.7/dist-packages (from libzero==0.0.4) (1.12.1+cu113)\n",
            "Requirement already satisfied: numpy<2,>=1.17 in /usr/local/lib/python3.7/dist-packages (from libzero==0.0.4) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.6->libzero==0.0.4) (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "# Requirements:\n",
        "!pip install rtdl\n",
        "!pip install libzero==0.0.4"
      ],
      "id": "IGuoK2CCdiH8"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ehSUeaAT7TwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d27649d7-7194-4572-ea4b-409529e3e7ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ConfigSpace\n",
            "  Downloading ConfigSpace-0.6.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 29.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ConfigSpace) (1.21.6)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from ConfigSpace) (0.29.32)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from ConfigSpace) (4.1.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace) (3.0.9)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ConfigSpace) (1.7.3)\n",
            "Installing collected packages: ConfigSpace\n",
            "Successfully installed ConfigSpace-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ConfigSpace"
      ],
      "id": "ehSUeaAT7TwO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwTQag8A7Udh"
      },
      "source": [
        "#Auxiliary classes "
      ],
      "id": "YwTQag8A7Udh"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "78FOYnnJAhAs"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Iterable\n",
        "\n",
        "import numpy as np\n",
        "import openml\n",
        "import pandas as pd\n",
        "from openml import OpenMLDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "task_ids = (\n",
        "    40588,\n",
        "    40589,\n",
        "    40590,\n",
        "    40591,\n",
        "    40592,\n",
        "    40593,\n",
        "    40594,\n",
        "    40595,\n",
        "    40596,\n",
        "    40597,\n",
        ")\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Split:\n",
        "    X: np.ndarray\n",
        "    y: np.ndarray\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Dataset:\n",
        "    name: str\n",
        "    id: int\n",
        "    features: pd.DataFrame\n",
        "    labels: pd.DataFrame\n",
        "    openml: OpenMLDataset\n",
        "    encoders: dict[str, LabelEncoder]\n",
        "\n",
        "    def split(\n",
        "        self,\n",
        "        splits: Iterable[float],\n",
        "        seed: int | None = 1,\n",
        "    ) -> tuple[Split, ...]:\n",
        "        \"\"\"Create splits of the data\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        splits : Iterable[float]\n",
        "            The percentages of splits to generate\n",
        "\n",
        "        seed : int | None = None\n",
        "            The seed to use for the splits\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple[Split, ...]\n",
        "            The collected splits\n",
        "        \"\"\"\n",
        "        splits = list(splits)\n",
        "        assert abs(1 - sum(splits)) <= 1e-6, \"Splits must sum to 1\"\n",
        "\n",
        "        sample_sizes = tuple(int(s * len(self.features)) for s in splits)\n",
        "\n",
        "        collected_splits = []\n",
        "\n",
        "        next_xs = self.features.to_numpy()\n",
        "        next_ys = self.labels.to_numpy()\n",
        "\n",
        "        for size in sample_sizes[:-1]:\n",
        "            xs, next_xs, ys, next_ys = train_test_split(\n",
        "                next_xs, next_ys, train_size=size, random_state=seed\n",
        "            )\n",
        "            collected_splits.append(Split(X=xs, y=ys))\n",
        "        collected_splits.append(Split(X=next_xs, y=next_ys))\n",
        "\n",
        "        return tuple(collected_splits)\n",
        "\n",
        "    @staticmethod\n",
        "    def from_openml(id: int) -> Dataset:\n",
        "        \"\"\"Processes an multilabel OpenMLDataset into its features and targets\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        id: int\n",
        "            The id of the dataset\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Dataset\n",
        "        \"\"\"\n",
        "        dataset = openml.datasets.get_dataset(id)\n",
        "        print(dataset.name, id)\n",
        "        targets = dataset.default_target_attribute.split(\",\")\n",
        "        data, _, _, _ = dataset.get_data()\n",
        "\n",
        "        assert isinstance(data, pd.DataFrame)\n",
        "\n",
        "        # Process the features and turn all categorical columns into ints\n",
        "        features = data.drop(columns=targets)\n",
        "        encoders: dict[str, LabelEncoder] = {}\n",
        "\n",
        "        for name, col in features.iteritems():\n",
        "            if col.dtype in [\"object\", \"category\", \"string\"]:\n",
        "                encoder = LabelEncoder()\n",
        "                features[name] = encoder.fit_transform(col)\n",
        "                encoders[name] = encoder\n",
        "\n",
        "        labels = data[targets]\n",
        "\n",
        "        # Since we assume binary multilabel data, we convert the labels\n",
        "        # to all be boolean types\n",
        "        labels = labels.astype(bool)\n",
        "\n",
        "        return Dataset(\n",
        "            name=dataset.name,\n",
        "            id=id,\n",
        "            features=features,\n",
        "            labels=labels,\n",
        "            openml=dataset,\n",
        "            encoders=encoders,\n",
        "        )\n",
        "\n",
        "from sklearn.metrics import f1_score as sklearn_f1_score\n",
        "import numpy as np\n",
        "\n",
        "def f1_score(y_true: np.ndarray, y_pred:np.ndarray) -> float:\n",
        "    return sklearn_f1_score(y_true, y_pred, average=\"macro\", zero_division=0)"
      ],
      "id": "78FOYnnJAhAs"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fdd7b91e"
      },
      "outputs": [],
      "source": [
        "# mlp for multi-label classification\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# get the dataset\n",
        "def get_dataset():\n",
        "\tX, y = make_multilabel_classification(n_samples=1000, n_features=10, n_classes=3, n_labels=2, random_state=1)\n",
        "\treturn X, y\n",
        "\n",
        "# get the model\n",
        "def get_model(n_inputs, n_outputs):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
        "\tmodel.add(Dense(20, kernel_initializer='he_uniform', activation='relu'))\n",
        "\tmodel.add(Dense(n_outputs, activation='sigmoid'))\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\treturn model\n",
        "\n",
        "# evaluate a model using repeated k-fold cross-validation\n",
        "def evaluate_model(X, y):\n",
        "\tresults = list()\n",
        "\tn_inputs, n_outputs = X.shape[1], y.shape[1]\n",
        "\t# define evaluation procedure\n",
        "\tcv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\t# enumerate folds\n",
        "\tfor train_ix, test_ix in cv.split(X):\n",
        "\t\t# prepare data\n",
        "\t\tX_train, X_test = X[train_ix], X[test_ix]\n",
        "\t\ty_train, y_test = y[train_ix], y[test_ix]\n",
        "\t\t# define model\n",
        "\t\tmodel = get_model(n_inputs, n_outputs)\n",
        "\t\t# fit model\n",
        "\t\tmodel.fit(X_train, y_train, verbose=0, epochs=100)\n",
        "\t\t# make a prediction on the test set\n",
        "\t\tyhat = model.predict(X_test)\n",
        "\t\t# round probabilities to class labels\n",
        "\t\tyhat = yhat.round()\n",
        "\t\t# calculate accuracy\n",
        "\t\tacc = f1_score(y_test, yhat)\n",
        "\t\t# store result\n",
        "\t\tprint('>%.3f' % acc)\n",
        "\t\tresults.append(acc)\n",
        "\treturn results\n",
        "\n"
      ],
      "id": "fdd7b91e"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edWaUa8kwuEH",
        "outputId": "6f8383f2-3f9a-4d3e-b29c-1376b6305a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import pandas as pd\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "id": "edWaUa8kwuEH"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cxiXzs_415CK"
      },
      "outputs": [],
      "source": [
        "from torch.functional import split\n",
        "class MyDataset(Dataset):\n",
        " \n",
        "  def __init__(self,split):\n",
        " \n",
        "    x=np.array(split.X)\n",
        "    y=np.array(split.y)\n",
        " \n",
        "    self.x_train=torch.tensor(x,dtype=torch.float32)\n",
        "    self.y_train=torch.tensor(y,dtype=torch.float32)\n",
        " \n",
        "  def __len__(self):\n",
        "    return len(self.y_train)\n",
        "   \n",
        "  def __getitem__(self,idx):\n",
        "    return self.x_train[idx],self.y_train[idx]"
      ],
      "id": "cxiXzs_415CK"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GT0hxXsDwsuQ"
      },
      "outputs": [],
      "source": [
        "def train_(dataloader, model, loss_fn, optimizer,logger,dataset_test=None,ftt=False):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # Compute prediction error\n",
        "        pred = model(X,None) if ftt else model(X) \n",
        "        loss = loss_fn(pred, y)\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch % 100 == 0:\n",
        "          yhat = torch.sigmoid(pred).round()\n",
        "          f1_train = f1_score(y.detach().cpu().numpy(), yhat.detach().cpu().numpy())\n",
        "          if(dataset_test is not None):\n",
        "            f1_test = test(dataset_test,model,loss_fn,ftt)\n",
        "            logger.log({\"loss\":loss,\"F1_train\":f1_train,\"F1_val\":f1_test})\n",
        "          else:\n",
        "            logger.log({\"loss\":loss,\"F1_train\":f1_train})\n",
        "        del X,y,pred,loss\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer,dataset_test=None,ftt=False):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # Compute prediction error\n",
        "        pred = model(X,None) if ftt else model(X) \n",
        "        loss = loss_fn(pred, y)\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "       \n",
        "\n",
        "def test(dataloader, model, loss_fn,ftt=False):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    #print(\"num batches: \"+str(num_batches))\n",
        "    #print(\"size: \"+str(size))\n",
        "    model.eval()\n",
        "    f1 = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X,None) if ftt else  model(X ) \n",
        "            pred = torch.sigmoid(pred)\n",
        "            yhat = pred.round()\n",
        "            # calculate accuracy\n",
        "            f1 += f1_score(y.detach().cpu().numpy(), yhat.detach().cpu().numpy())\n",
        "    f1 /= num_batches\n",
        "    return f1\n",
        "    "
      ],
      "id": "GT0hxXsDwsuQ"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9zzFnXXM5ZW5"
      },
      "outputs": [],
      "source": [
        "def get_best_thresholds(dataloader,model,ftt=False):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      for X, y in dataloader:\n",
        "          X, y = X.to(device), y.to(device)\n",
        "          pred = model(X,None) if ftt else  model(X)\n",
        "          pred = torch.sigmoid(pred)\n",
        "  final_thresholds = np.array([])\n",
        "  thresholds = np.array([.20,.40,.50,.60,80])\n",
        "  for i in range(y.shape[1]):\n",
        "    pred_ =pred[:,i]\n",
        "    y_ = y[:,i]\n",
        "    scores = np.array([])\n",
        "    for threshold in thresholds:\n",
        "      yhat = (pred_>threshold).float()\n",
        "      score = f1_score(yhat.detach().cpu().numpy(),y_.detach().cpu().numpy())\n",
        "      scores = np.append(scores,score)\n",
        "    index = np.argmax(scores)\n",
        "    final_thresholds = np.append(final_thresholds,thresholds[index])\n",
        "  return final_thresholds"
      ],
      "id": "9zzFnXXM5ZW5"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "v2mxto2LtQaA"
      },
      "outputs": [],
      "source": [
        "def test_with_thresholds(dataloader, model, loss_fn,thresholds,ftt=False):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    f1 = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X,None) if ftt else  model(X) \n",
        "            pred = torch.sigmoid(pred)\n",
        "            yhat = None\n",
        "            for i in range(len(thresholds)):\n",
        "              pred_ =pred[:,i]\n",
        "              yhat_ = (pred_>thresholds[i]).float()\n",
        "              if yhat== None:\n",
        "                yhat = yhat_ \n",
        "              else: \n",
        "                yhat = torch.column_stack((yhat,yhat_))\n",
        "            f1 = f1_score(y.detach().cpu().numpy(), yhat.detach().cpu().numpy())\n",
        "    f1 /= num_batches\n",
        "    return f1\n"
      ],
      "id": "v2mxto2LtQaA"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a66cCmbcZs-9",
        "outputId": "09d20de5-e4b1-4ff9-f184-0c9302e24e0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total    : 15843721216\n",
            "free     : 15840575488\n",
            "used     : 3145728\n"
          ]
        }
      ],
      "source": [
        "from pynvml import *\n",
        "nvmlInit()\n",
        "h = nvmlDeviceGetHandleByIndex(0)\n",
        "info = nvmlDeviceGetMemoryInfo(h)\n",
        "print(f'total    : {info.total}')\n",
        "print(f'free     : {info.free}')\n",
        "print(f'used     : {info.used}')"
      ],
      "id": "a66cCmbcZs-9"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvEsfg1xsUSE",
        "outputId": "b4fa75c2-5acf-4c20-82fa-bb5daa7976e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (first_layer): Linear(in_features=2, out_features=512, bias=True)\n",
            "  (blocks): Sequential(\n",
            "    (0): Block(\n",
            "      (normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (linear_first): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (activation): ReLU()\n",
            "      (dropout_first): Dropout(p=0.35, inplace=False)\n",
            "      (linear_second): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (dropout_second): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (1): Block(\n",
            "      (normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (linear_first): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (activation): ReLU()\n",
            "      (dropout_first): Dropout(p=0.35, inplace=False)\n",
            "      (linear_second): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (dropout_second): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (2): Block(\n",
            "      (normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (linear_first): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (activation): ReLU()\n",
            "      (dropout_first): Dropout(p=0.35, inplace=False)\n",
            "      (linear_second): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (dropout_second): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (3): Block(\n",
            "      (normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (linear_first): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (activation): ReLU()\n",
            "      (dropout_first): Dropout(p=0.35, inplace=False)\n",
            "      (linear_second): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (dropout_second): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (4): Block(\n",
            "      (normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (linear_first): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (activation): ReLU()\n",
            "      (dropout_first): Dropout(p=0.35, inplace=False)\n",
            "      (linear_second): Linear(in_features=512, out_features=512, bias=True)\n",
            "      (dropout_second): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (head): Head(\n",
            "    (normalization): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (activation): ReLU()\n",
            "    (linear): Linear(in_features=512, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import rtdl\n",
        "x = torch.randn(4, 2)\n",
        "config = { \"d_in\":x.shape[1],\n",
        "    \"n_blocks\":5,\n",
        "    \"d_main\":512,\n",
        "    \"d_hidden\":512,\n",
        "    \"dropout_first\":0.35,\n",
        "    \"dropout_second\":0.0,\n",
        "    \"d_out\":1}\n",
        "module = rtdl.ResNet.make_baseline(**config)\n",
        "assert module(x).shape == (len(x), 1)\n",
        "\n",
        "print(module)"
      ],
      "id": "xvEsfg1xsUSE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FynifhSy7hpv"
      },
      "source": [
        "#Data Preprocesing "
      ],
      "id": "FynifhSy7hpv"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "poqSM9xu7miV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import sys\n",
        "import time\n",
        "import re\n",
        "import math\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "class Dataset_():\n",
        "    def __init__(self, dataset_number, threshold=0.7, seed = 0) -> None:\n",
        "        self.features_name = []\n",
        "        self.features_type = []\n",
        "        self.features_possible_values = []\n",
        "        self.features_unique_value = []\n",
        "        self.data = []\n",
        "        self.random_generator = np.random.RandomState(seed)\n",
        "        self.threshold = threshold\n",
        "        self.dataset_number_to_name(dataset_number)\n",
        "        \n",
        "        self.initialization()\n",
        "    \n",
        "    def load_dataset(self):\n",
        "        \n",
        "        # Get path\n",
        "        absolute_path = os.getcwd()\n",
        "        full_path = os.path.join(absolute_path, \"gdrive/MyDrive/datasets\", self.dataset_name+\".arff\")\n",
        "        \n",
        "        # Read file\n",
        "        with open(full_path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "        \n",
        "        # Load dataset by reading file line by line\n",
        "        state = 0\n",
        "        for line in lines:\n",
        "            strings = line[:-1].split(\" \", 2)\n",
        "            if strings[0] == \"@relation\":\n",
        "                print(\"Begin to read dataset\")\n",
        "                \n",
        "            elif strings[0] == \"@attribute\":\n",
        "                self.features_name.append(strings[1][1:-1])\n",
        "                if strings[2][0] == \"{\" and strings[2][-1] == \"}\":\n",
        "                    if len(strings[2].split(\",\")) == 2:\n",
        "                        self.features_type.append(\"binary\")\n",
        "                    else:\n",
        "                        self.features_type.append(\"categorical\")\n",
        "                else:\n",
        "                    self.features_type.append(strings[2])\n",
        "                \n",
        "                if self.features_type[-1] == \"categorical\" or self.features_type[-1] == \"binary\":\n",
        "                    values = strings[2][1:-1].replace(\" \", \"\")\n",
        "                    values = values.replace(\"'\", \"\")\n",
        "                    values = values.split(\",\")\n",
        "                    self.features_possible_values.append(values)\n",
        "                else :\n",
        "                    self.features_possible_values.append(\"endless\")\n",
        "                \n",
        "            elif strings[0] == \"@data\":\n",
        "                # From now Read data\n",
        "                state = 1\n",
        "            \n",
        "            elif state == 1:\n",
        "                tmp_line = line[:-1].replace('\"', \"\")\n",
        "                tmp_line = tmp_line.replace(' ', \"\")\n",
        "                values = tmp_line.split(\",\")\n",
        "                sample = []\n",
        "                for i in range(len(values)):\n",
        "                    value = values[i]\n",
        "                    if self.features_type[i] == \"numeric\":\n",
        "                        sample.append(float(value))\n",
        "                    \n",
        "                    elif self.features_type[i] == \"binary\" or self.features_type[i] == \"categorical\":\n",
        "                        value = self.features_possible_values[i].index(value)\n",
        "                        sample.append(float(value))\n",
        "                self.data.append(sample)\n",
        "            else:\n",
        "                assert False, \"Got to invalid case: state \"+str(state)+\" first string \"+strings[0]                                \n",
        "                \n",
        "                \n",
        "        self.data = np.array(self.data)\n",
        "        for i in np.arange(self.data.shape[1]):\n",
        "            self.features_unique_value.append(np.unique(self.data[:,i]))\n",
        "            if len(self.features_unique_value[i]) == 2 and self.features_type == \"numeric\":\n",
        "                self.features_type[i] = \"binary\"\n",
        "                self.features_possible_values = self.features_unique_value\n",
        "                for j in range(self.data.shape[0]):\n",
        "                    self.data[j,i] = float(self.features_possible_values[i].index(value))\n",
        "                \n",
        "\n",
        "        \n",
        "    def check_for_single_value_parameters(self):\n",
        "        \n",
        "        params_to_delete = []\n",
        "        for i in np.arange(self.data.shape[1]-1, -1, -1):\n",
        "            if self.features_name[i] in self.target_features:\n",
        "                continue\n",
        "            unique_values = self.features_unique_value[i]\n",
        "            if unique_values.shape[0] == 1:\n",
        "                params_to_delete.append(i)\n",
        "\n",
        "        old = self.data.shape[1]\n",
        "        self.data = np.delete(self.data, params_to_delete, 1)\n",
        "        self.clear_feature_data(params_to_delete)\n",
        "        new = self.data.shape[1]\n",
        "        print(\"Removed \",old - new,\" parameters with one unique value\")\n",
        "        #print(params_to_delete)\n",
        "    \n",
        "    def check_for_pseudocorrelation(self):\n",
        "        \n",
        "        params_to_delete = []\n",
        "        for i in np.arange(self.data.shape[1]-1, -1, -1):\n",
        "            if self.features_name[i] in self.target_features or self.features_type[i] == \"numeric\":\n",
        "                continue\n",
        "            for j in np.arange(i-1, -1, -1):\n",
        "                if self.features_name[j] in self.target_features or self.features_type[j] == \"numeric\":\n",
        "                    continue\n",
        "                # Check for pseudocorrelation\n",
        "                temp_i = self.data[:,i]\n",
        "                temp_j = self.data[:,j]\n",
        "                uniq_val_num_i = len(self.features_unique_value[i])\n",
        "                uniq_val_num_j = len(self.features_unique_value[j])\n",
        "                if uniq_val_num_i > uniq_val_num_j:\n",
        "                    lower = j\n",
        "                else:\n",
        "                    lower = i\n",
        "                temp_i = temp_i * uniq_val_num_j\n",
        "                sum = temp_i + temp_j\n",
        "                uniq_val = np.unique(sum)\n",
        "                if uniq_val.shape[0] == uniq_val_num_i or uniq_val.shape[0] == uniq_val_num_j:\n",
        "                    if not(lower in params_to_delete):\n",
        "                        params_to_delete.append(lower)\n",
        "                        #print(self.features_name[i],\" \",self.features_name[j],\" \",lower)\n",
        "                        \n",
        "                    #break\n",
        "\n",
        "        old = self.data.shape[1]\n",
        "        self.data = np.delete(self.data, params_to_delete, 1)\n",
        "        self.clear_feature_data(params_to_delete)\n",
        "        new = self.data.shape[1]\n",
        "        print(\"Removed \",old - new,\" parameters with pseudocorrelated values\")\n",
        "        #print(params_to_delete)\n",
        "              \n",
        "              \n",
        "    def check_for_correlation(self):\n",
        "        params_to_delete = []\n",
        "        for i in range(len(self.features_type)):\n",
        "            if self.features_name[i] in self.target_features or self.features_type[i] != \"numeric\":\n",
        "                continue\n",
        "            for j in range(len(self.features_type)):\n",
        "                if self.features_name[j] in self.target_features or self.features_type[j] == \"numeric\":\n",
        "                    continue\n",
        "                \n",
        "                sum_i = np.sum(self.data[:, i])\n",
        "                sum_j = np.sum(self.data[:, j])\n",
        "                sum_i2 = np.sum(self.data[:, i]**2)\n",
        "                sum_j2 = np.sum(self.data[:, j]**2)\n",
        "                sum_ij = np.sum(self.data[:, i] * self.data[:, j])\n",
        "                n = self.data.shape[0]\n",
        "                \n",
        "                correlation = (n*sum_ij - sum_i * sum_j)/(math.sqrt((n * sum_i2 - (sum_i)**2)*(n * sum_j2 - (sum_j)**2)))\n",
        "                correlation = abs(correlation)\n",
        "                if correlation>= self.threshold:\n",
        "                    if len(self.features_unique_value[i])>=len(self.features_unique_value[j]):\n",
        "                        if not(j in params_to_delete):\n",
        "                            params_to_delete.append(j)\n",
        "                    else:\n",
        "                        if not(i in params_to_delete):\n",
        "                            params_to_delete.append(i)\n",
        "                \n",
        "        old = self.data.shape[1]\n",
        "        self.data = np.delete(self.data, params_to_delete, 1)\n",
        "        self.clear_feature_data(params_to_delete)\n",
        "        new = self.data.shape[1]\n",
        "        print(\"Removed \",old - new,\" parameters with correlated values\")\n",
        "        #print(params_to_delete)\n",
        "                 \n",
        "    def clear_feature_data(self, indices:list):\n",
        "        \n",
        "        indices.sort()\n",
        "        indices = indices[::-1]\n",
        "        for i in indices:\n",
        "            self.features_name.pop(i)\n",
        "            self.features_type.pop(i)\n",
        "            self.features_possible_values.pop(i)\n",
        "            self.features_unique_value.pop(i)   \n",
        "        \n",
        "        \n",
        "    def find_targets(self):\n",
        "        \n",
        "        if self.dataset_name == \"birds\":\n",
        "            pattern = r\"^[A-Z][a-z]\"\n",
        "            number_of_targets = 19\n",
        "        elif self.dataset_name == \"emotions\":\n",
        "            pattern = r\"[a-z]\\.[a-z]\"\n",
        "            number_of_targets = 6\n",
        "        elif self.dataset_name == \"enron\":\n",
        "            pattern = r\"[A-Z]\\.[A-Z][0-9]+\"\n",
        "            number_of_targets = 53\n",
        "        elif self.dataset_name == \"genbase\":\n",
        "            pattern = r\"PDOC\"\n",
        "            number_of_targets = 27\n",
        "        elif self.dataset_name == \"image\":\n",
        "            pattern = r\"^((?!Feature).)*$\"\n",
        "            number_of_targets = 5\n",
        "        elif self.dataset_name == \"langLog\":\n",
        "            pattern = r\"^((?!tok[0-9]+).)*$\"\n",
        "            number_of_targets = 75\n",
        "        elif self.dataset_name == \"reuters\":\n",
        "            pattern = r\"^label[1-7]\"\n",
        "            number_of_targets = 7\n",
        "        elif self.dataset_name == \"scene\":\n",
        "            pattern = \"^((?!Att[0-9]).)*$\"\n",
        "            number_of_targets = 6\n",
        "        elif self.dataset_name == \"slashdot\":\n",
        "            pattern = \"^[A-Z][a-zA-Z]\"\n",
        "            number_of_targets = 22\n",
        "        elif self.dataset_name == \"yeast\":\n",
        "            pattern = r\"Class[1-9]\"\n",
        "            number_of_targets = 14\n",
        "        else: \n",
        "            assert False, \"Wrong dataset name : \"+self.dataset_name\n",
        "        \n",
        "        self.target_features = []\n",
        "        for i in range(self.data.shape[1]-1, -1, -1):\n",
        "            decision = re.search(pattern, self.features_name[i])\n",
        "            if decision:\n",
        "                self.target_features.append(self.features_name[i])\n",
        "                self.features_name\n",
        "        self.number_of_targets = number_of_targets\n",
        "        assert len(self.target_features) == self.number_of_targets, \"Found number of targets \"+str(len(self.target_features))+\" differs from true number \"+str(self.number_of_targets) \n",
        "        \n",
        "        \n",
        "    def check_for_values_with_one_occurance(self):\n",
        "        \n",
        "        params_to_delete = []\n",
        "        for i in np.arange(self.data.shape[1]-1, -1, -1):\n",
        "            if self.features_name[i] in self.target_features:\n",
        "                continue\n",
        "            if self.features_type[i] == \"numeric\":\n",
        "                continue\n",
        "            temp = (self.data[:,i]).astype(int)\n",
        "            occurances = np.bincount(temp)\n",
        "            if 1 in occurances:\n",
        "                params_to_delete.append(i)\n",
        "\n",
        "        old = self.data.shape[1]\n",
        "        self.data = np.delete(self.data, params_to_delete, 1)\n",
        "        self.clear_feature_data(params_to_delete)\n",
        "        new = self.data.shape[1]\n",
        "        print(\"Removed \",old - new,\" parameters with 1 occurance of value\")\n",
        "        #print(params_to_delete)\n",
        "    \n",
        "    \n",
        "    def check_for_duplicated_samples(self):\n",
        "        \n",
        "        old = self.data.shape[0]\n",
        "        self.data = np.unique(self.data, axis=0)\n",
        "        new = self.data.shape[0]\n",
        "        print(\"Removed \",old - new,\" samples due to being duplicates\")\n",
        "    \n",
        "    \n",
        "    def normalize_dataset(self):\n",
        "        \n",
        "        for i in range(self.data.shape[1]):\n",
        "            if self.features_name in self.target_features:\n",
        "                continue\n",
        "            if not self.features_type == \"numeric\":\n",
        "                continue\n",
        "            \n",
        "            average = np.mean(self.data[:, i])\n",
        "            \n",
        "            self.data[:, i] = self.data[:, i] - average\n",
        "            \n",
        "            maximum = np.amax(np.absolute(self.data[:,i]))\n",
        "            \n",
        "            self.data[:, i] = self.data[:, i]/maximum\n",
        "            \n",
        "    \n",
        "    def dataset_split(self, train_ratio : float):\n",
        "        \n",
        "        perm_idxs = np.arange(self.data.shape[0])\n",
        "        self.random_generator.shuffle(perm_idxs)\n",
        "        split_idx = int(self.data.shape[0]*train_ratio)\n",
        "        train_idxs = perm_idxs[:split_idx]\n",
        "        valid_idxs = perm_idxs[split_idx:]\n",
        "        \n",
        "        train_pred_data = self.predictors[train_idxs]\n",
        "        train_tar_data = self.targets[train_idxs]\n",
        "        valid_pred_data = self.predictors[valid_idxs]\n",
        "        valid_tar_data = self.targets[valid_idxs]\n",
        "        return train_pred_data, train_tar_data, valid_pred_data, valid_tar_data\n",
        "        \n",
        "       \n",
        "    def dataset_to_predictors_targets(self):\n",
        "        \n",
        "        self.predictors = []\n",
        "        self.targets = []\n",
        "        self.predictors_features_type = []\n",
        "        self.targets_features_type = []\n",
        "        \n",
        "        for i in range(self.data.shape[1]):\n",
        "            if self.features_name[i] in self.target_features:\n",
        "                self.targets.append(self.data[:,i])\n",
        "                self.targets_features_type.append(self.features_type[i])\n",
        "            else:\n",
        "                self.predictors.append(self.data[:,i])\n",
        "                self.predictors_features_type.append(self.features_type[i])\n",
        "                \n",
        "        self.predictors = np.array(self.predictors)\n",
        "        self.targets = np.array(self.targets)\n",
        "        self.targets = np.swapaxes(self.targets, 0, 1)\n",
        "        self.predictors = np.swapaxes(self.predictors, 0, 1)\n",
        "        \n",
        "    \n",
        "    def dataset_number_to_name(self, number):\n",
        "        dataset_names = [\"birds\", \"emotions\", \"enron\", \"genbase\",\n",
        "                         \"image\", \"langLog\", \"reuters\", \"scene\",\n",
        "                         \"slashdot\", \"yeast\"]\n",
        "        \n",
        "        assert number < len(dataset_names), \"Chosen number \"+str(number)+\" is outside possible values 0-\"+str(len(dataset_names)-1)\n",
        "        self.dataset_name = dataset_names[number]\n",
        "        print(dataset_names[number])\n",
        "            \n",
        "        \n",
        "    def one_hot_encoding(self, x):\n",
        "        length = x.shape[0]\n",
        "        max_value = int(np.max(x))\n",
        "        new_array = np.zeros((length, max_value+1))\n",
        "        new_array[np.arange(length), x.astype(int)] = 1.0\n",
        "        return new_array\n",
        "    \n",
        "    \n",
        "    def dataset_to_one_hot_encoding(self):\n",
        "        for i in range(len(self.predictors_features_type)-1, -1, -1):\n",
        "            if self.predictors_features_type[i] == \"categorical\":\n",
        "                ar = self.one_hot_encoding(self.predictors[:,i])\n",
        "            else:\n",
        "                ar = self.predictors[:,i]\n",
        "                ar = np.reshape(ar, (-1, 1))\n",
        "\n",
        "            if i+1 == len(self.predictors_features_type):\n",
        "                self.predictors = np.concatenate((self.predictors[:,:i], ar),axis=1)\n",
        "            else:\n",
        "                self.predictors = np.concatenate((self.predictors[:,:i], ar, self.predictors[:,i+1:]),axis=1)\n",
        "        \n",
        "        for i in range(len(self.targets_features_type)-1, -1, -1):\n",
        "            if self.targets_features_type[i] == \"categorical\":\n",
        "                ar = self.one_hot_encoding(self.targets[:,i])\n",
        "            else:\n",
        "                ar = self.targets[:,i]\n",
        "                ar = np.reshape(ar, (-1, 1))\n",
        "            if i+1 == len(self.targets_features_type):\n",
        "                self.targets = np.concatenate((self.targets[:,:i], ar),axis=1)\n",
        "            else:\n",
        "                self.targets = np.concatenate((self.targets[:,:i], ar, self.targets[:,i+1:]),axis=1)\n",
        "            \n",
        "    def initialization(self):\n",
        "        start = time.process_time()\n",
        "        self.load_dataset()\n",
        "        self.find_targets()\n",
        "        self.check_for_single_value_parameters()\n",
        "        self.check_for_values_with_one_occurance()\n",
        "        self.check_for_duplicated_samples()\n",
        "        self.check_for_pseudocorrelation()\n",
        "        self.check_for_correlation()\n",
        "        self.normalize_dataset()\n",
        "        self.dataset_to_predictors_targets()\n",
        "        self.dataset_to_one_hot_encoding()\n",
        "        \n",
        "        print(\"It took \",time.process_time() - start)\n",
        "        print(\"Final shape of the dataset (predictors + tagrets) is \",self.data.shape)\n",
        "        print(\"Number of predictors : \", self.data.shape[1] - len(self.target_features), \" and number of targets : \",len(self.target_features))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DatasetSampler():\n",
        "    def __init__(self, train_pred, train_target, valid_pred, valid_target, train_bs = 64, valid_bs = 64, seed = 0, reshuffle = False, mode = \"train\") -> None:\n",
        "        self.train_pred = train_pred\n",
        "        self.train_target = train_target\n",
        "        self.train_bs = train_bs\n",
        "        \n",
        "        self.valid_pred = valid_pred\n",
        "        self.valid_target = valid_target\n",
        "        self.valid_bs = valid_bs\n",
        "        \n",
        "        self.random_generator = np.random.RandomState(seed)\n",
        "        self.reshuffle = reshuffle\n",
        "        \n",
        "        self.train_iter = 0\n",
        "        self.valid_iter = 0\n",
        "        \n",
        "        self.train_iter_max = math.ceil(self.train_pred.shape[0] / self.train_bs)\n",
        "        self.valid_iter_max = math.ceil(self.valid_pred.shape[0] / self.valid_bs)\n",
        "        \n",
        "        assert mode == \"train\" or mode == \"valid\", \"Chosen mode is not train or valid. Chosen mode is\"+mode\n",
        "        self.mode = mode\n",
        "        \n",
        "        \n",
        "    def sample_batch(self, pred, target, bs, iter):\n",
        "             \n",
        "        pred_batch = pred[iter*bs : min((iter+1)*bs ,pred.shape[0])]\n",
        "        target_batch = target[iter*bs : min((iter+1)*bs ,pred.shape[0])]\n",
        "        return pred_batch, target_batch \n",
        "    \n",
        "    \n",
        "    def get_batch(self):\n",
        "                \n",
        "        if self.mode == \"train\":\n",
        "            if self.train_iter == self.train_iter_max :\n",
        "                if self.reshuffle:\n",
        "                    perm = np.arange(self.train_pred.shape[0])\n",
        "                    self.random_generator.shuffle(perm)\n",
        "                    self.train_pred = self.train_pred[perm]\n",
        "                    self.train_target = self.train_target[perm]\n",
        "                self.train_iter = 0\n",
        "                raise StopIteration\n",
        "                \n",
        "            pred_batch, target_batch = self.sample_batch(self.train_pred, self.train_target, self.train_bs, self.train_iter)  \n",
        "            self.train_iter = self.train_iter + 1\n",
        "            \n",
        "        elif self.mode == \"valid\":\n",
        "            if self.valid_iter == self.valid_iter_max :\n",
        "                if self.reshuffle:\n",
        "                    perm = np.arange(self.valid_pred.shape[0])\n",
        "                    self.random_generator.shuffle(perm)\n",
        "                    self.valid_pred = self.valid_pred[perm]\n",
        "                    self.valid_target = self.valid_target[perm]\n",
        "                self.valid_iter = 0\n",
        "                raise StopIteration\n",
        "            \n",
        "            pred_batch, target_batch = self.sample_batch(self.valid_pred, self.valid_target, self.valid_bs, self.valid_iter)\n",
        "            self.valid_iter = self.valid_iter + 1\n",
        "            \n",
        "        return pred_batch, target_batch\n",
        "        \n",
        "    \n",
        "    def reset_iterators(self):\n",
        "        self.train_iter = 0\n",
        "        self.valid_iter = 0\n",
        "        \n",
        "    \n",
        "    def set_mode(self, mode):\n",
        "        assert mode == \"train\" or mode == \"valid\", \"Chosen mode is not train or valid. Chosen mode is\"+mode\n",
        "        self.mode = mode\n",
        "        \n",
        "    def __iter__(self):\n",
        "        return self\n",
        "    \n",
        "    \n",
        "    def __next__(self):\n",
        "        return self.get_batch()\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        assert self.mode != \"train\" or self.train_iter_max >= index, \"Index outside range :\"+str(self.train_iter_max)+\" got \"+str(index)\n",
        "        assert self.mode != \"valid\" or self.valid_iter_max >= index, \"Index outside range :\"+str(self.valid_iter_max)+\" got \"+str(index)\n",
        "        \n",
        "        if self.mode == \"train\":\n",
        "            temp = self.train_iter\n",
        "            to_return = self.get_batch()\n",
        "            self.train_iter = temp\n",
        "            \n",
        "        elif self.mode == \"valid\":\n",
        "            temp = self.valid_iter\n",
        "            to_return = self.get_batch()\n",
        "            self.valid_iter = temp\n",
        "            \n",
        "        else:\n",
        "            assert False, \"Invalid mode :\"+self.mode\n",
        "            \n",
        "        return to_return\n",
        "            \n",
        "    def __len__(self):\n",
        "        if self.mode == \"train\":\n",
        "            to_return = self.train_iter_max\n",
        "            \n",
        "        elif self.mode == \"valid\":\n",
        "            to_return = self.valid_iter_max\n",
        "            \n",
        "        else:\n",
        "            assert False, \"Invalid mode :\"+self.mode\n",
        "    \n",
        "        return to_return\n"
      ],
      "id": "poqSM9xu7miV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DE"
      ],
      "metadata": {
        "id": "TmtzG9jkc64P"
      },
      "id": "TmtzG9jkc64P"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import rtdl\n",
        "import wandb\n"
      ],
      "metadata": {
        "id": "cNVEp-YVjrw-"
      },
      "id": "cNVEp-YVjrw-",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def obj(x,budget,dataset_id):\n",
        "\n",
        "  seed = 42\n",
        "\n",
        "  loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "  dataset = Dataset.from_openml(dataset_id)\n",
        "  train_split, test_split = dataset.split(splits=(0.75, 0.25), seed=seed)\n",
        "  n_inputs, n_outputs = len(dataset.features.columns), len(dataset.labels.columns)\n",
        "\n",
        "  name = \"ResNet_DEHB_\" + dataset.name\n",
        "  config = { \"d_in\":n_inputs,\n",
        "    \"n_blocks\":x[\"n_blocks\"],\n",
        "    \"d_main\":x[\"d_main\"],\n",
        "    \"d_hidden\":x[\"d_hidden\"],\n",
        "    \"dropout_first\":x[\"dropout_first\"],\n",
        "    \"dropout_second\":x[\"dropout_second\"],\n",
        "    \"d_out\":n_outputs}\n",
        "\n",
        "\n",
        "  model = rtdl.ResNet.make_baseline(**config).to(device)  \n",
        "\n",
        "  config[\"batch_size\"] = x[\"batch_size\"]\n",
        "  config[\"epoch\"] = budget\n",
        "\n",
        "  print(config)\n",
        "\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), lr=x['lr'], weight_decay=x['weight_decay'])\n",
        "\n",
        "  train_dataset = MyDataset(train_split)\n",
        "  test_dataset = MyDataset(test_split)\n",
        "\n",
        "  train_dataloader = DataLoader(train_dataset, batch_size=x[\"batch_size\"])\n",
        "  test_dataloader = DataLoader(test_dataset, batch_size=x[\"batch_size\"])\n",
        "\n",
        "  for t in range(budget):\n",
        "    train(train_dataloader, model, loss_fn, optimizer,ftt=False,dataset_test=test_dataloader)\n",
        "  \n",
        "  score_test = test(test_dataloader,model, loss_fn,ftt=False)\n",
        "\n",
        "  del model\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  print(\"f1_score: \" + str(score_test))\n",
        "  \n",
        "  return {\"f1_score\": score_test}"
      ],
      "metadata": {
        "id": "uxJqYFWxgQsu"
      },
      "id": "uxJqYFWxgQsu",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import time\n",
        "from typing import Callable, Union\n",
        "\n",
        "import ConfigSpace\n",
        "import numpy as np\n",
        "from ConfigSpace.util import (deactivate_inactive_hyperparameters,\n",
        "                              impute_inactive_values)\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "\n",
        "class ConfigVectorSpace(ConfigSpace.ConfigurationSpace):\n",
        "\n",
        "    def __init__(self, *args, **kwargs) -> None:\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.hyperparameters = self.get_hyperparameters()\n",
        "        self.dim = len(self.hyperparameters)\n",
        "\n",
        "        self.name_to_idx = {}\n",
        "        for i, hp in enumerate(self.hyperparameters):\n",
        "            # maps hyperparameter name to positional index in vector form\n",
        "            self.name_to_idx[hp.name] = i\n",
        "        \n",
        "    def sample_vectors(self, size):\n",
        "        configurations = super().sample_configuration(size)\n",
        "        if not isinstance(configurations , list):\n",
        "            configurations = [configurations]\n",
        "\n",
        "        vectors = [self._to_vector(config) for config in configurations]\n",
        "        return vectors\n",
        "    \n",
        "    def _to_vector(self, config: ConfigSpace.Configuration) -> np.array:\n",
        "        '''Converts ConfigSpace.Configuration object to numpy array scaled to [0,1]\n",
        "        Works when self is a ConfigVectorSpace object and the input config is a ConfigSpace.Configuration object.\n",
        "        Handles conditional spaces implicitly by replacing illegal parameters with default values\n",
        "        to maintain the dimensionality of the vector.\n",
        "        '''        \n",
        "        config = impute_inactive_values(config)\n",
        "\n",
        "        vector = [None] * self.dim\n",
        "        self.name_to_id = dict()\n",
        "\n",
        "        for name in config:\n",
        "            idx = self.name_to_idx[name]\n",
        "            hyper = self[name]\n",
        "            if type(hyper) == ConfigSpace.OrdinalHyperparameter:\n",
        "                nlevels = len(hyper.sequence)\n",
        "                vector[idx] = hyper.sequence.index(config[name]) / nlevels\n",
        "            elif type(hyper) == ConfigSpace.CategoricalHyperparameter:\n",
        "                nlevels = len(hyper.choices)\n",
        "                vector[idx] = hyper.choices.index(config[name]) / nlevels\n",
        "            else:\n",
        "                bounds = (hyper.lower, hyper.upper)\n",
        "                param_value = config[name]\n",
        "                if hyper.log:\n",
        "                    vector[idx] = np.log(param_value / bounds[0]) / np.log(bounds[1] / bounds[0])\n",
        "                else:\n",
        "                    vector[idx] = (config[name] - bounds[0]) / (bounds[1] - bounds[0])\n",
        "        return np.array(vector)\n",
        "    \n",
        "    def to_config(self, vector: np.array) -> ConfigSpace.Configuration:\n",
        "        '''Converts numpy array to ConfigSpace.Configuration object\n",
        "        Works when self is a ConfigVectorSpace object and the input vector is in the domain [0, 1].\n",
        "        '''\n",
        "        # creates a ConfigSpace object dict with all hyperparameters present, the inactive too\n",
        "        new_config = impute_inactive_values(self.sample_configuration()).get_dictionary()\n",
        "\n",
        "        # iterates over all hyperparameters and normalizes each based on its type\n",
        "        for hyper in self.hyperparameters:\n",
        "            idx = self.name_to_idx[hyper.name]\n",
        "\n",
        "            if type(hyper) == ConfigSpace.OrdinalHyperparameter:\n",
        "                ranges = np.arange(start=0, stop=1, step=1/len(hyper.sequence))\n",
        "                param_value = hyper.sequence[np.where((vector[idx] < ranges) == False)[0][-1]]\n",
        "            elif type(hyper) == ConfigSpace.CategoricalHyperparameter:\n",
        "                ranges = np.arange(start=0, stop=1, step=1/len(hyper.choices))\n",
        "                param_value = hyper.choices[np.where((vector[idx] < ranges) == False)[0][-1]]\n",
        "            else:  # handles UniformFloatHyperparameter & UniformIntegerHyperparameter\n",
        "                # rescaling continuous values\n",
        "                if hyper.log:\n",
        "                    log_range = np.log(hyper.upper) - np.log(hyper.lower)\n",
        "                    param_value = np.exp(np.log(hyper.lower) + vector[idx] * log_range)\n",
        "                else:\n",
        "                    param_value = hyper.lower + (hyper.upper - hyper.lower) * vector[idx]\n",
        "                if type(hyper) == ConfigSpace.UniformIntegerHyperparameter:\n",
        "                    param_value = int(np.round(param_value))  # converting to discrete (int)\n",
        "                else:\n",
        "                    param_value = float(param_value)\n",
        "            new_config[hyper.name] = param_value\n",
        "\n",
        "        #self.check_configuration(ConfigSpace.Configuration(self, values=new_config))\n",
        "        #new_config = impute_inactive_values(ConfigSpace.Configuration(self, values=new_config))\n",
        "        # the mapping from unit hypercube to the actual config space may lead to illegal\n",
        "        # configurations based on conditions defined, which need to be deactivated/removed\n",
        "        new_config = deactivate_inactive_hyperparameters(\n",
        "            configuration = new_config, configuration_space=self\n",
        "        )\n",
        "        return new_config\n",
        "\n",
        "\n",
        "class DE(object):\n",
        "    def __init__(self,\n",
        "        space : ConfigVectorSpace,\n",
        "        crossover_prob : float = 0.9,\n",
        "        mutation_factor : float = 0.8,\n",
        "        metric = \"f1_score\",\n",
        "        mode = \"max\",\n",
        "        rs: np.random.RandomState=None,\n",
        "        bound_control = \"random\",\n",
        "        save_path=\".\") -> None:\n",
        "\n",
        "        assert 0 <= mutation_factor <= 2, ValueError(\"mutation_factor not in range [0, 2]\")\n",
        "        assert mode in [\"min\", \"max\"], ValueError(\"Valid optimization mode in ['min', 'max']\")\n",
        "        \n",
        "        self.space = space\n",
        "        self.crossover_prob = crossover_prob\n",
        "        self.mutation_factor = mutation_factor\n",
        "        self.metric = metric\n",
        "        self.mode = mode\n",
        "        self.rs = rs\n",
        "        self.bound_control = bound_control\n",
        "        self.save_path=save_path\n",
        "        \n",
        "        self.traj = []\n",
        "        self.inc_config = None\n",
        "        self.inc_score = float(\"inf\") if self.mode == \"min\" else float(\"-inf\")\n",
        "\n",
        "        self.histroy = []\n",
        "\n",
        "        self._min_pop_size = 3\n",
        "        self._eval_counter = -1\n",
        "        self._iteration_counter = -1\n",
        "\n",
        "    def _init_population(self, pop_size : int) -> np.ndarray :\n",
        "\n",
        "        # sample from ConfigSpace s.t. conditional constraints (if any) are maintained\n",
        "        population = self.space.sample_vectors(size=pop_size)\n",
        "        return np.asarray(population)\n",
        "    \n",
        "    def _eval_population(self, obj : Callable, population: Union[np.ndarray, list] , budget, **kwargs) -> np.ndarray:\n",
        "        fitness = []\n",
        "\n",
        "        for candidate in population:\n",
        "            config = self.space.to_config(candidate)\n",
        "            result = obj(config, budget, **kwargs)\n",
        "\n",
        "            assert isinstance(result, dict), TypeError(\"Objective function must return a dictionary\")\n",
        "            assert self.metric in result, KeyError(f\"Given mteric '{self.metric}' not found in dictionary {result}, returned by the objective function\")\n",
        "            score = result[self.metric]\n",
        "\n",
        "            condition = {\n",
        "                \"min\" : score < self.inc_score,\n",
        "                \"max\" : score > self.inc_score, \n",
        "            } \n",
        "\n",
        "            if condition[self.mode]:\n",
        "                self.inc_config = config\n",
        "                self.inc_score = score\n",
        "\n",
        "            fitness.append(score)\n",
        "            \n",
        "            self.traj.append(self.inc_score)\n",
        "            self._update_history(candidate, result, budget)\n",
        "            self._eval_counter += 1\n",
        "\n",
        "        return np.asarray(fitness)\n",
        "\n",
        "    def _update_history(self, candidate, result, budget):\n",
        "        record = {\n",
        "            \"candidate\": candidate.tolist(),\n",
        "            \"budget\": budget}\n",
        "        \n",
        "        record.update(result)\n",
        "        self.histroy.append(record)\n",
        "\n",
        "    def _sample(self, population, size, replace=False):\n",
        "        selection = self.rs.choice(np.arange(len(population)), size, replace=replace)\n",
        "        return population[selection]\n",
        "\n",
        "    def _mutation(self, population: np.ndarray):\n",
        "        pop_size = len(population)\n",
        "        assert pop_size > self._min_pop_size, ValueError(f\"Population too small ( < DE()._min_pop_size = {self._min_pop_size}) for mutation\")\n",
        "\n",
        "        base, a, b = self._sample(population, self._min_pop_size)\n",
        "\n",
        "        diff = a - b\n",
        "        mutant = base + self.mutation_factor * diff\n",
        "        return mutant\n",
        "\n",
        "    def _crossover(self, candidate: np.ndarray, mutant: np.ndarray) -> np.ndarray:\n",
        "        # Sample a random value from U[0, 1] for every dimension\n",
        "        p = self.rs.rand(self.space.dim)\n",
        "\n",
        "        # perform binomial crossover\n",
        "        child = np.asarray([mutant[i] if p[i] < self.crossover_prob else candidate[i] for i in range(self.space.dim)])\n",
        "        return child\n",
        "    \n",
        "    def _selection(self, population: np.ndarray, children: np.ndarray, fitness: np.ndarray, children_fitness: np.ndarray):\n",
        "        # Conduct parent-child competition and select new population \n",
        "        pop_size = len(population)\n",
        "        for i in range(pop_size):\n",
        "            condition = {\n",
        "                \"max\" : children_fitness[i] >= fitness[i],\n",
        "                \"min\" : children_fitness[i] <= fitness[i]}\n",
        "            if condition[self.mode]:\n",
        "                population[i] = children[i]\n",
        "                fitness[i] = children_fitness[i]\n",
        "        \n",
        "        return population, fitness\n",
        "    \n",
        "    def _check_bounds(self, vector: np.ndarray) -> np.ndarray:\n",
        "\n",
        "        violations = np.where((vector > 1) | (vector < 0))[0]\n",
        "        if len(violations) == 0:\n",
        "            return vector\n",
        "        if self.bound_control == 'random':\n",
        "            vector[violations] = self.rs.uniform(low=0.0, high=1.0, size=len(violations))\n",
        "        else:\n",
        "            # Can be exploited by optimizer if solution at clip limits \n",
        "            vector[violations] = np.clip(vector[violations], a_min=0, a_max=1)\n",
        "        return vector\n",
        "\n",
        "    def _next_generation(self, population, alt_pop=None):\n",
        "        children = []\n",
        "\n",
        "        for candidate in population:\n",
        "            # If alt_pop is None, vanilla mutation is perfomed\n",
        "            mutant = self._mutation(alt_pop if alt_pop is not None else population)\n",
        "            mutant = self._check_bounds(mutant)\n",
        "            child = self._crossover(candidate, mutant)\n",
        "            children.append(child)\n",
        "        \n",
        "        return children\n",
        "\n",
        "    def optimize(self, obj : Callable, budget=None, pop_size : Union[int,None] = None, limit: int = 10, unit : str = \"iter\", **kwargs):\n",
        "        self._start_timer()\n",
        "\n",
        "        if pop_size is None:\n",
        "            pop_size = 10 * self.space.dim # heuristic\n",
        "\n",
        "        # Initialize and Evaluate the population\n",
        "        population = self._init_population(pop_size)\n",
        "        fitness = self._eval_population(obj, population, budget, **kwargs)\n",
        "\n",
        "        # Until budget is exhausted\n",
        "        while not self._is_termination(limit, unit):\n",
        "            self._iteration_counter += 1\n",
        "\n",
        "            children = self._next_generation(population)\n",
        "\n",
        "            children_fitness = self._eval_population(obj, children, budget)\n",
        "            population, fitness = self._selection(population, children, fitness, children_fitness)\n",
        "        \n",
        "        return self.inc_config\n",
        "    \n",
        "    def _start_timer(self):\n",
        "        self._wall_clock_start = time.time()\n",
        "\n",
        "    def _is_termination(self, limit : int, unit : str):\n",
        "        assert unit in [\"hr\", \"min\", \"sec\", \"evals\", \"iter\"], ValueError(\"Unrecognized unit given\")\n",
        "\n",
        "        if unit in [\"hr\", \"min\", \"sec\"]:\n",
        "            scale = {\n",
        "                \"sec\" : 1,\n",
        "                \"min\" : 60,\n",
        "                \"hr\"  : 60 * 60\n",
        "            }\n",
        "            diff = time.time() - self._wall_clock_start\n",
        "            return diff >= limit * scale[unit]\n",
        "        elif unit == \"evals\":\n",
        "            return self._eval_counter + 1 > limit\n",
        "        else:\n",
        "            return self._iteration_counter + 1 > limit\n",
        "\n",
        "    def save_data(self):\n",
        "        data = {\n",
        "            \"params\" : self._init_params(),\n",
        "            \"result\" : {\n",
        "                \"best_config\": self.inc_config.get_dictionary(),\n",
        "                \"best_score\" : self.inc_score,\n",
        "            },\n",
        "            \"traj\": self.traj,\n",
        "            \"history\" : self.histroy,\n",
        "        }\n",
        "\n",
        "        with open(os.path.join(self.save_path, \"data.json\"), \"w\") as outfile:\n",
        "            json.dump(data, outfile)\n",
        "    \n",
        "    def _init_params(self):\n",
        "        params = {\n",
        "            \"crossover_prob\" : self.crossover_prob,\n",
        "            \"mutation_factor\" : self.mutation_factor,\n",
        "            \"metric\" : self.metric,\n",
        "            \"mode\" : self.mode,\n",
        "            \"seed\" : SEED,\n",
        "            \"bound_control\" : self.bound_control,\n",
        "            \"iters\" : self._iteration_counter,\n",
        "            \"evals\" : self._eval_counter\n",
        "\n",
        "        }\n",
        "        return params\n",
        "\n",
        "class DEHB(DE):\n",
        "    def __init__(self, \n",
        "        space: ConfigVectorSpace,\n",
        "        min_budget : int = 100,\n",
        "        max_budget : int = 1000,\n",
        "        eta : int = 2,\n",
        "        crossover_prob: float = 0.9, \n",
        "        mutation_factor: float = 0.8, \n",
        "        metric = \"f1_score\",\n",
        "        mode = \"max\",\n",
        "        rs: np.random.RandomState=None,\n",
        "        bound_control = \"random\") -> None:\n",
        "\n",
        "        super().__init__(space=space,\n",
        "            crossover_prob=crossover_prob, \n",
        "            mutation_factor=mutation_factor,\n",
        "            metric=metric,\n",
        "            mode=mode,\n",
        "            bound_control=bound_control, \n",
        "            rs=rs)\n",
        "        \n",
        "        self.min_budget = min_budget\n",
        "        self.max_budget = max_budget\n",
        "\n",
        "        self.eta = eta\n",
        "        \n",
        "        self._all_in_one = self._get_bracket()\n",
        "        self._SH_iter = len(self._all_in_one)\n",
        "\n",
        "        self._genus = None\n",
        "    \n",
        "    def _get_bracket(self):\n",
        "        s_max = int(np.floor(np.log(self.max_budget / self.min_budget) / np.log(self.eta)))\n",
        "\n",
        "        budgets = (self.max_budget * np.power(self.eta,-np.linspace(start=s_max, stop=0, num=s_max + 1))).tolist()\n",
        "        budgets = list(map(int, budgets))\n",
        "        \n",
        "        N = int(np.ceil(self.eta ** s_max))\n",
        "        n_configs = [max(int(N*(self.eta**(-i))), 1) for i in range(s_max + 1)]\n",
        "\n",
        "        bracket = tuple(zip(n_configs, budgets))\n",
        "        print(bracket)\n",
        "        return bracket\n",
        "    \n",
        "    def _init_eval_genus(self, obj : Callable, **kwargs):\n",
        "        genus = dict()\n",
        "\n",
        "        for (pop_size, budget) in self._all_in_one:\n",
        "            species = dict()\n",
        "            species[\"population\"] = self._init_population(pop_size)\n",
        "            species[\"fitness\"] = self._eval_population(obj, species[\"population\"], budget, **kwargs)\n",
        "            \n",
        "            #genus[budget] = species\n",
        "            genus[budget] = self._sort_species(species)\n",
        "        return genus\n",
        "    \n",
        "    def _sort_species(self, species : np.ndarray):\n",
        "        species = species.copy()\n",
        "        ranking = np.argsort(species[\"fitness\"])\n",
        "        if self.mode == \"max\":\n",
        "            ranking = ranking[::-1]\n",
        "        \n",
        "        species[\"population\"] = species[\"population\"][ranking]\n",
        "        species[\"fitness\"] = species[\"fitness\"][ranking]\n",
        "\n",
        "        return species\n",
        "\n",
        "    def _select_promotions(self, target : dict, previous : dict):\n",
        "        promotions = []\n",
        "        pop_size = len(target[\"population\"])\n",
        "\n",
        "        for individual in previous[\"population\"]:\n",
        "            # If individual already in target, then ignore it to minimize fn_evals\n",
        "            if np.any(np.all(individual == target[\"population\"], axis=1)):\n",
        "                continue\n",
        "            \n",
        "            promotions.append(individual)\n",
        "        \n",
        "        if len(promotions) >= pop_size:\n",
        "            promotions = promotions[:pop_size]\n",
        "        else:\n",
        "            return previous[\"population\"][:pop_size]\n",
        "            # raise BufferError(\"Not enough to promote\")\n",
        "            # Can simply pick top pop_size many individuals even \n",
        "            # if duplicates exist in target\n",
        "        \n",
        "        return np.asarray(promotions)\n",
        "    \n",
        "    def _get_alt_population(self, target : dict, previous : dict):\n",
        "\n",
        "        # stage == 0, previous is None\n",
        "        if previous is None:\n",
        "            \n",
        "            # Edge case where stage==0, but target population too small\n",
        "            # for vanilla mutation, so we need an alt_pop that is not None\n",
        "            if len(target) < self._min_pop_size :\n",
        "                previous = target\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "        pop_size = len(target[\"population\"])\n",
        "        alt_pop = previous[\"population\"][:pop_size]\n",
        "\n",
        "        if len(alt_pop) < self._min_pop_size:\n",
        "            filler_size = self._min_pop_size - len(alt_pop) + 1\n",
        "            filler_pop = self._sample(self.global_pupulation, filler_size)\n",
        "\n",
        "            alt_pop = np.concatenate([filler_pop, alt_pop])\n",
        "        \n",
        "        return alt_pop\n",
        "    \n",
        "    def optimize(self, obj : Callable, limit : int = 10, unit : str = \"iter\", **kwargs):\n",
        "        self._start_timer()\n",
        "\n",
        "        self.genus = self._init_eval_genus(obj, **kwargs)\n",
        "\n",
        "        while not self._is_termination(limit, unit): # DEHB iteration\n",
        "            self._iteration_counter += 1\n",
        "\n",
        "            for j in range(self._SH_iter): # SH iterations\n",
        "                \n",
        "                previous = None\n",
        "                bracket = self._all_in_one[j:]\n",
        "\n",
        "                for stage, (pop_size, budget) in enumerate(bracket): # stages in a bracket\n",
        "                    target =  self.genus[budget]\n",
        "\n",
        "                    # Only True for first DEHB iteration and non-inital SH stage\n",
        "                    promotion = True if self._iteration_counter == 0 and stage > 0 else False\n",
        "                    if promotion:\n",
        "                        children = self._select_promotions(target, previous)\n",
        "                    else:\n",
        "                        alt_pop = self._get_alt_population(target, previous)\n",
        "                        children = self._next_generation(target[\"population\"], alt_pop)\n",
        "\n",
        "                    children_fitness = self._eval_population(obj, children, budget, **kwargs)\n",
        "                    target[\"population\"], target[\"fitness\"] = self._selection(target[\"population\"], children, target[\"fitness\"], children_fitness)\n",
        "\n",
        "                    target = self._sort_species(target)\n",
        "                    self.genus[budget] = target\n",
        "                    previous = target\n",
        "        \n",
        "        return self.inc_config\n",
        "            \n",
        "    def _init_params(self):\n",
        "        params = {\n",
        "            \"min_budget\" : self.min_budget,\n",
        "            \"max_budget\" : self.max_budget,\n",
        "            \"eta\"  : self.eta\n",
        "        }\n",
        "        params.update(super()._init_params())\n",
        "        print(params)\n",
        "        return params\n",
        "\n",
        "    @property\n",
        "    def global_pupulation(self):\n",
        "        assert self.genus is not None, AssertionError(\"No populaiton in genus initialized\")\n",
        "        return np.concatenate([species[\"population\"] for species in self.genus.values()])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    rs = np.random.RandomState(seed=SEED)\n",
        "    \n",
        "    space = ConfigVectorSpace(\n",
        "        name=\"neuralnetwork\",\n",
        "        seed=SEED,\n",
        "        # TODO : find distribution for drop_0, drop_1 and reg_const\n",
        "        # SUGGESTIONS : drop_0 and drop_1 conditional on layer, as deeper layers need larger dropout\n",
        "        space={\n",
        "            \"lr\": ConfigSpace.UniformFloatHyperparameter(\"lr\", lower=1e-6, upper=1e-1, log=True, default_value=1e-3),\n",
        "            \"dropout_first\": ConfigSpace.Float('dropout_first', bounds=(0, 0.9), default=0.34, distribution=ConfigSpace.Normal(mu=0.5, sigma=0.35)),\n",
        "            \"dropout_second\": ConfigSpace.Float('dropout_second', bounds=(0, 0.9), default=0.34, distribution=ConfigSpace.Normal(mu=0.5, sigma=0.35)),\n",
        "            \"weight_decay\": ConfigSpace.Float(\"weight_decay\", bounds=(0, 5), default=0.1),\n",
        "            \"n_blocks\": ConfigSpace.UniformIntegerHyperparameter(\"n_blocks\", lower=2, upper=6, default_value=2),\n",
        "            \"batch_size\": ConfigSpace.OrdinalHyperparameter(\"batch_size\", sequence=[64, 128, 512], default_value=64),\n",
        "            \"d_main\": ConfigSpace.OrdinalHyperparameter(\"d_main\", sequence=[32, 64, 128,256,512], default_value=128),\n",
        "            \"d_hidden\" : ConfigSpace.OrdinalHyperparameter(\"d_hidden\", sequence=[64, 128, 256, 512], default_value=64)\n",
        "        },\n",
        "    )\n",
        "\n",
        "    dehb = DEHB(space, min_budget=50, max_budget=1000, rs=rs)\n",
        "\n",
        "    start_time = time.process_time()\n",
        "\n",
        "    print(f\"Best configuration  {dehb.optimize(obj, limit=25,  unit='min', dataset_id=40588)}\")\n",
        "    print(f\"Time elapsed (CPU time): {(time.process_time() - start_time):.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xtjoOTi52Ft",
        "outputId": "3a0ee6c7-354c-4176-a815-a7536024a5f6"
      },
      "id": "0xtjoOTi52Ft",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "((16, 62), (8, 125), (4, 250), (2, 500), (1, 1000))\n",
            "birds 40588\n",
            "{'d_in': 260, 'n_blocks': 5, 'd_main': 32, 'd_hidden': 128, 'dropout_first': 0.8377374000639999, 'dropout_second': 0.2342863598269771, 'd_out': 19, 'batch_size': 512, 'epoch': 62}\n",
            "f1_score: 0.07812442555404611\n",
            "birds 40588\n",
            "{'d_in': 260, 'n_blocks': 3, 'd_main': 128, 'd_hidden': 512, 'dropout_first': 0.8568935677327879, 'dropout_second': 0.861572991799429, 'd_out': 19, 'batch_size': 64, 'epoch': 62}\n",
            "f1_score: 0.11011404673254614\n",
            "birds 40588\n",
            "{'d_in': 260, 'n_blocks': 2, 'd_main': 128, 'd_hidden': 512, 'dropout_first': 0.7047907898390531, 'dropout_second': 0.677783945323129, 'd_out': 19, 'batch_size': 512, 'epoch': 62}\n",
            "f1_score: 0.01754385964912281\n",
            "birds 40588\n",
            "{'d_in': 260, 'n_blocks': 3, 'd_main': 64, 'd_hidden': 128, 'dropout_first': 0.3356264456132211, 'dropout_second': 0.8274302466944439, 'd_out': 19, 'batch_size': 512, 'epoch': 62}\n",
            "f1_score: 0.11330099440605342\n",
            "birds 40588\n",
            "{'d_in': 260, 'n_blocks': 3, 'd_main': 128, 'd_hidden': 128, 'dropout_first': 0.1437860349937251, 'dropout_second': 0.7818108194421902, 'd_out': 19, 'batch_size': 64, 'epoch': 62}\n",
            "f1_score: 0.0\n",
            "birds 40588\n",
            "{'d_in': 260, 'n_blocks': 5, 'd_main': 128, 'd_hidden': 128, 'dropout_first': 0.6085928150000488, 'dropout_second': 0.5466556205453351, 'd_out': 19, 'batch_size': 64, 'epoch': 62}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "992c99fb",
        "YwTQag8A7Udh",
        "FynifhSy7hpv"
      ],
      "name": "DEHB_ResNet.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}